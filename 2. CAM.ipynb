{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import shutil\n",
    "from scipy.ndimage import zoom\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import io\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader,random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix, classification_report,precision_recall_curve,roc_curve,accuracy_score, ConfusionMatrixDisplay\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCAM3D:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "\n",
    "        self.hook()\n",
    "\n",
    "    def hook(self):\n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output\n",
    "\n",
    "        def backward_hook(module, grad_in, grad_out):\n",
    "            self.gradients = grad_out[0]\n",
    "\n",
    "        self.target_layer.register_forward_hook(forward_hook)\n",
    "        self.target_layer.register_backward_hook(backward_hook)\n",
    "\n",
    "    def generate_cam(self, input_tensor, target_class):\n",
    "        self.model.eval()\n",
    "        output = self.model(input_tensor)\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        target = output.squeeze()\n",
    "        target.backward()\n",
    "\n",
    "        # Extract gradients and activations\n",
    "        gradients = self.gradients.cpu().data.numpy()[0]\n",
    "        activations = self.activations.cpu().data.numpy()[0]\n",
    "\n",
    "        # Compute weights\n",
    "        weights = np.mean(gradients, axis=(1, 2, 3))\n",
    "        cam = np.zeros(activations.shape[1:], dtype=np.float32)\n",
    "\n",
    "        for i, w in enumerate(weights):\n",
    "            cam += w * activations[i, :, :, :]\n",
    "\n",
    "        cam = np.maximum(cam, 0)  # ReLU\n",
    "\n",
    "        # Upsample CAM to match original input size\n",
    "        cam_tensor = torch.tensor(cam, dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # Add batch and channel dims\n",
    "        cam_upsampled = F.interpolate(\n",
    "            cam_tensor, size=input_tensor.shape[2:], mode=\"trilinear\", align_corners=False\n",
    "        )  # Match original size\n",
    "        cam_upsampled = cam_upsampled.squeeze().cpu().numpy()\n",
    "\n",
    "        return cam_upsampled / np.max(cam_upsampled)  # Normalize to [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BasicBlock3D 정의\n",
    "class BasicBlock3D(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None, dropout_prob=0.3):\n",
    "        super(BasicBlock3D, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
    "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "        self.dropout = nn.Dropout3d(p=dropout_prob)  # Dropout 추가\n",
    "\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.dropout(out)  # Dropout 적용\n",
    "        out = self.bn2(self.conv2(out))\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        return F.relu(out)\n",
    "\n",
    "\n",
    "# ResNet18-3D 정의\n",
    "class ResNet3D(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=1):\n",
    "        super(ResNet3D, self).__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        # 초기 Conv 레이어\n",
    "        self.conv1 = nn.Conv3d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # ResNet Layers\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        # Global Average Pooling과 FC 레이어\n",
    "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv3d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm3d(out_channels * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = [block(self.in_channels, out_channels, stride, downsample)]\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# ResNet18-3D 생성 함수\n",
    "def resnet18_3d():\n",
    "    return ResNet3D(BasicBlock3D, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "def register_hooks_for_tensorboard(model, writer):\n",
    "    def hook_fn(module, input, output):\n",
    "        # Step을 TensorBoard에서 사용하기 위해 전역 변수 가져오기\n",
    "        global global_step\n",
    "\n",
    "        # Conv3D나 BatchNorm3D의 출력 feature를 이미지로 저장\n",
    "        if output.dim() == 5:  # 3D 텐서 [batch, channel, depth, height, width]\n",
    "            # 첫 번째 샘플(batch)만 확인\n",
    "            feature_map = output[0]  # Shape: [channel, depth, height, width]\n",
    "\n",
    "            # 첫 번째 채널의 중간 depth 슬라이스를 저장\n",
    "            middle_slice = feature_map[0, feature_map.shape[1] // 2, :, :]  # Shape: [height, width]\n",
    "            writer.add_image(f\"{module._get_name()}_feature_map\", middle_slice.cpu().detach().numpy(), global_step, dataformats=\"HW\")\n",
    "\n",
    "            # 전체 채널의 평균을 계산하여 대표 슬라이스 저장 (2D로 축소)\n",
    "            mean_feature = torch.mean(feature_map, dim=0)  # Shape: [depth, height, width]\n",
    "            mean_slice = mean_feature[mean_feature.shape[0] // 2, :, :]  # Shape: [height, width]\n",
    "            writer.add_image(f\"{module._get_name()}_mean_feature_map\", mean_slice.cpu().detach().numpy(), global_step, dataformats=\"HW\")\n",
    "\n",
    "    # Hook 등록: Conv3D, BatchNorm3D, BasicBlock3D 레이어 대상\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (nn.Conv3d, nn.BatchNorm3d, BasicBlock3D)):\n",
    "            module.register_forward_hook(hook_fn)\n",
    "\n",
    "\n",
    "# 학습 루프\n",
    "def train_model(model, train_loader, criterion, optimizer, writer, device, epochs):\n",
    "    global global_step\n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, (volumes, scores) in enumerate(train_loader):\n",
    "            global_step += 1\n",
    "\n",
    "            volumes, scores = volumes.to(device), scores.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(volumes)\n",
    "            loss = criterion(outputs.squeeze(), scores)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # TensorBoard에 Loss 기록\n",
    "            writer.add_scalar(\"Loss/Train\", loss.item(), global_step)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nii 파일 로드 + 전처리 함수들\n",
    "# NIfTI 파일 로드\n",
    "def load_nii(file_path):\n",
    "    try:\n",
    "        data = nib.load(file_path).get_fdata(dtype=np.float32)\n",
    "        if data.ndim == 4 and data.shape[-1] == 1:\n",
    "            data = data.squeeze(axis=-1)\n",
    "        elif data.ndim != 3:\n",
    "            raise ValueError(f\"Unexpected data dimensions: {data.shape}. Expected 3D data.\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading NIfTI file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# PyTorch 기반 리사이즈 함수\n",
    "def resize_volume_torch(volume, target_shape=(256, 256, 256)):\n",
    "    try:\n",
    "        volume_tensor = torch.tensor(volume, dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # Add batch and channel dims\n",
    "        resized = F.interpolate(volume_tensor, size=target_shape, mode='trilinear', align_corners=False)\n",
    "        return resized.squeeze().numpy()\n",
    "    except Exception as e:\n",
    "        print(f\"Error resizing volume: {e}\")\n",
    "        return None\n",
    "\n",
    "# 단일 파일 처리\n",
    "def process_file(file_name, mmse_score, base_path, target_shape):\n",
    "    file_path = os.path.join(base_path, file_name)\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "\n",
    "    volume = load_nii(file_path)\n",
    "    if volume is None:\n",
    "        return None\n",
    "\n",
    "    resized_volume = resize_volume_torch(volume, target_shape)\n",
    "    if resized_volume is None:\n",
    "        return None\n",
    "\n",
    "    return [resized_volume, mmse_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def visualize_cam(original, cam, output_dir, slice_axis=2):\n",
    "#     output_dir = Path(output_dir)\n",
    "#     output_dir.mkdir(parents=True, exist_ok=True)\n",
    "#     # 텐서를 numpy로 변환\n",
    "#     original = original.cpu().squeeze().numpy() if isinstance(original, torch.Tensor) else original\n",
    "#     print(f\"cam : {cam.shape}\")\n",
    "#     cam = cam.cpu().squeeze().numpy() if isinstance(cam, torch.Tensor) else cam\n",
    "#     num_slices = original.shape[slice_axis]\n",
    "#     for i in range(num_slices):\n",
    "#         if slice_axis == 0:\n",
    "#             original_slice = original[i, :, :]\n",
    "#             cam_slice = cam[i, :, :]\n",
    "#         elif slice_axis == 1:\n",
    "#             original_slice = original[:, i, :]\n",
    "#             cam_slice = cam[:, i, :]\n",
    "#         else:\n",
    "#             original_slice = original[:, :, i]\n",
    "#             cam_slice = cam[:, :, i]\n",
    "\n",
    "#         overlay = original_slice * 0.5 + cam_slice * 0.5\n",
    "\n",
    "#         plt.figure(figsize=(15, 5))\n",
    "#         plt.subplot(1, 3, 1)\n",
    "#         plt.title(\"Original\")\n",
    "#         plt.imshow(original_slice, cmap=\"gray\")\n",
    "\n",
    "#         plt.subplot(1, 3, 2)\n",
    "#         plt.title(\"CAM\")\n",
    "#         plt.imshow(cam_slice, cmap=\"jet\")\n",
    "\n",
    "#         plt.subplot(1, 3, 3)\n",
    "#         plt.title(\"Overlay\")\n",
    "#         plt.imshow(overlay, cmap=\"hot\")\n",
    "\n",
    "#         plt.savefig(output_dir / f\"slice_{i:03d}.png\")\n",
    "#         plt.close()\n",
    "\n",
    "def normalize_to_range(data, min_val=0, max_val=1):\n",
    "    data_min = np.min(data)\n",
    "    data_max = np.max(data)\n",
    "    if data_max - data_min == 0:  # Avoid division by zero\n",
    "        return np.zeros_like(data)\n",
    "    return (data - data_min) / (data_max - data_min) * (max_val - min_val) + min_val\n",
    "\n",
    "def visualize_cam(original, cam, output_dir, slice_axis=2):\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 텐서를 numpy로 변환\n",
    "    original = original.cpu().squeeze().numpy() if isinstance(original, torch.Tensor) else original\n",
    "    cam = cam.cpu().squeeze().numpy() if isinstance(cam, torch.Tensor) else cam\n",
    "\n",
    "    num_slices = original.shape[slice_axis]\n",
    "    for i in range(num_slices):\n",
    "        if slice_axis == 0:\n",
    "            original_slice = normalize_to_range(original[i, :, :])\n",
    "            cam_slice = cam[i, :, :]\n",
    "        elif slice_axis == 1:\n",
    "            original_slice = normalize_to_range(original[:, i, :])\n",
    "            cam_slice = cam[:, i, :]\n",
    "        else:\n",
    "            original_slice = normalize_to_range(original[:, :, i])\n",
    "            cam_slice = cam[:, :, i]\n",
    "\n",
    "        overlay = original_slice * 0.5 + cam_slice * 0.5\n",
    "\n",
    "        vmin = 0\n",
    "        vmax = 1\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(\"Original\")\n",
    "        plt.imshow(original_slice, cmap=\"gray\",vmin=vmin, vmax=vmax)\n",
    "        plt.colorbar()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(\"CAM\")\n",
    "        plt.imshow(cam_slice, cmap=\"jet\", vmin = vmin , vmax = vmax)\n",
    "        plt.colorbar()\n",
    "\n",
    "        # plt.subplot(1, 3, 3)\n",
    "        # plt.title(\"Overlay\")\n",
    "        # plt.imshow(overlay, cmap=\"jet\")\n",
    "\n",
    "        plt.savefig(output_dir / f\"slice_{i:03d}.png\")\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_33612\\124779372.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_path = 'resnet3d_18_epoch50_26_0_49.pth'\n",
    "nii_path = '2021-07-13,I1467526_resized.nii.gz'\n",
    "output_dir ='./CAM/12251337'\n",
    "\n",
    "model = resnet18_3d().to(device)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "target_layer = model.layer2[1].conv2\n",
    "cam_generator = GradCAM3D(model,target_layer)\n",
    "\n",
    "data = load_nii(nii_path)\n",
    "data = resize_volume_torch(data,(128,128,128))\n",
    "\n",
    "data = torch.tensor(data, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)  # [Batch, Channel, Depth, Height, Width]\n",
    "\n",
    "cam = cam_generator.generate_cam(data,target_class=0)\n",
    "\n",
    "plt.hist(cam[0].reshape(-1))\n",
    "plt.show()\n",
    "visualize_cam(data,cam,output_dir,slice_axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
