{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'zoom' from 'scipy.ndimage' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnibabel\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnib\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshutil\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mndimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m zoom\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Parallel, delayed\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'zoom' from 'scipy.ndimage' (unknown location)"
     ]
    }
   ],
   "source": [
    "# import\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import shutil\n",
    "from scipy.ndimage import zoom\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader,random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix, classification_report,precision_recall_curve,roc_curve,accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nii 파일 로드 + 전처리 함수들\n",
    "# NIfTI 파일 로드\n",
    "def load_nii(file_path):\n",
    "    try:\n",
    "        data = nib.load(file_path).get_fdata(dtype=np.float64)\n",
    "        if data.ndim == 4 and data.shape[-1] == 1:\n",
    "            data = data.squeeze(axis=-1)\n",
    "        elif data.ndim != 3:\n",
    "            raise ValueError(f\"Unexpected data dimensions: {data.shape}. Expected 3D data.\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading NIfTI file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# PyTorch 기반 리사이즈 함수\n",
    "def resize_volume_torch(volume, target_shape=(128, 128, 128)):\n",
    "    try:\n",
    "        volume_tensor = torch.tensor(volume, dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # Add batch and channel dims\n",
    "        resized = F.interpolate(volume_tensor, size=target_shape, mode='trilinear', align_corners=False)\n",
    "        return resized.squeeze().numpy()\n",
    "    except Exception as e:\n",
    "        print(f\"Error resizing volume: {e}\")\n",
    "        return None\n",
    "\n",
    "# 단일 파일 처리\n",
    "def process_file(file_name, mmse_score, base_path, target_shape):\n",
    "    file_path = os.path.join(base_path, file_name)\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "\n",
    "    volume = load_nii(file_path)\n",
    "    if volume is None:\n",
    "        return None\n",
    "\n",
    "    resized_volume = resize_volume_torch(volume, target_shape)\n",
    "    if resized_volume is None:\n",
    "        return None\n",
    "\n",
    "    return [resized_volume, mmse_score]\n",
    "\n",
    "# 데이터셋 생성 - Chunking + 병렬 처리\n",
    "def create_dataset(dataset, base_path, target_shape=(128, 128, 128), chunk_size=20):\n",
    "    processed_data = []\n",
    "    total_chunks = len(dataset) // chunk_size + (1 if len(dataset) % chunk_size != 0 else 0)\n",
    "\n",
    "    # tqdm으로 진행 상황 표시\n",
    "    with tqdm(total=total_chunks, desc=\"Processing Dataset\") as pbar:\n",
    "        for i in range(0, len(dataset), chunk_size):\n",
    "            chunk = dataset[i:i + chunk_size]\n",
    "            # 병렬 처리\n",
    "            processed_chunk = Parallel(n_jobs=4)(\n",
    "                delayed(process_file)(file_name, mmse_score, base_path, target_shape) for file_name, mmse_score in chunk\n",
    "            )\n",
    "            # 유효한 데이터만 추가\n",
    "            processed_data.extend([item for item in processed_chunk if item is not None])\n",
    "            # 메모리 디버깅 함수\n",
    "            # print(f\"{psutil.virtual_memory().percent}%\")\n",
    "            # Progress bar 업데이트\n",
    "            pbar.update(1)\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file 받아오고 처리하는 함수들\n",
    "def get_file_list(folder_path, extension=None):\n",
    "    try:\n",
    "        file_list = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f)) and (not extension or f.endswith(extension))]\n",
    "        return file_list\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting file list from {folder_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def rename_and_copy_files(folder_path, target_folder, extension=None):\n",
    "    try:\n",
    "        # 대상 폴더 생성\n",
    "        os.makedirs(target_folder, exist_ok=True)\n",
    "        file_list = get_file_list(folder_path, extension)\n",
    "        for file_name in file_list:\n",
    "            # 날짜 제거를 위한 파일 이름 분리\n",
    "            parts = file_name.split(\",\")\n",
    "            if len(parts) == 3:\n",
    "                new_name = f\"{parts[0]},{parts[2]}\"\n",
    "                old_path = os.path.join(folder_path, file_name)\n",
    "                new_path = os.path.join(target_folder, new_name)\n",
    "                shutil.copy(old_path, new_path)\n",
    "                print(f\"Copied and Renamed: {old_path} -> {new_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"파일 이름 변경 및 복사 중 에러 발생: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renamed 폴더로 이름에 환자번호 - mri번호만 남김 - 날짜 없는 파일에 대한 처리\n",
    "# folder_path = \"niis/ADNI4\"  # 폴더 경로를 적어주세요\n",
    "# target_folder = \"niis/ADNI4_renamed\"  # 복사할 대상 폴더 경로를 적어주세요\n",
    "# extension = \".gz\"  # 필터링할 확장자를 적어주세요 (예: \".gz\")\n",
    "# rename_and_copy_files(folder_path, target_folder, extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metadata_and_files(file_list, metadata, metadata_columns):\n",
    "    dataset = []\n",
    "    for file in file_list:\n",
    "        f = file\n",
    "        file_parts = file.split('.')[0].split(',')\n",
    "        if len(file_parts) < 2:\n",
    "            print(f\"Invalid file format: {file}\")\n",
    "            continue\n",
    "        \n",
    "        mmse = metadata.loc[\n",
    "            (metadata[metadata_columns[0]] == file_parts[0]) &\n",
    "            (metadata[metadata_columns[1]] == file_parts[1]), \n",
    "            'MMSCORE'\n",
    "        ]\n",
    "        \n",
    "        mmse_value = mmse.values[0] if not mmse.empty else None\n",
    "        if mmse_value is not None:\n",
    "            dataset.append([f, mmse_value])\n",
    "        else:\n",
    "            pass\n",
    "            # print(f\"No matching MMSE score for file: {file}\")\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 및 데이터 설정\n",
    "adni3_base_path = 'niis/ADNI3_renamed'\n",
    "adni4_base_path = 'niis/ADNI4_renamed'\n",
    "\n",
    "adni3_metadata = pd.read_csv('metadatas/adni3_metadata_cleaned.csv')\n",
    "adni4_metadata = pd.read_csv('metadatas/adni4_metadata_cleaned.csv')\n",
    "\n",
    "adni3_file_list = get_file_list(adni3_base_path, extension='.nii.gz')\n",
    "adni4_file_list = get_file_list(adni4_base_path, extension='.nii.gz')\n",
    "\n",
    "adni3_dataset = load_metadata_and_files(adni3_file_list, adni3_metadata, ['PTID', 'MRI Number'])\n",
    "adni4_dataset = load_metadata_and_files(adni4_file_list, adni4_metadata, ['PTID', 'MRI Number'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 생성\n",
    "target_shape = (128, 128, 128)\n",
    "train_dataset = create_dataset(adni3_dataset, adni3_base_path, target_shape)\n",
    "test_dataset = create_dataset(adni4_dataset, adni4_base_path, target_shape)\n",
    "# 디버깅용 출력\n",
    "for volume, score in train_dataset[:5]:\n",
    "    print(f\"Volume shape: {volume.shape}, MMSE Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in train_dataset[:3]:\n",
    "    for e in d:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ADNIDataset(Dataset):\n",
    "    def __init__(self, dataset, normalization=True, score_threshold=27, score_threshold2=23):\n",
    "        self.dataset = dataset\n",
    "        self.normalization = normalization\n",
    "        self.score_threshold = score_threshold\n",
    "        self.score_threshold2 = score_threshold2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        volume, score = self.dataset[idx]\n",
    "        volume = torch.tensor(volume, dtype=torch.float16)\n",
    "\n",
    "        # 정규화 처리\n",
    "        if self.normalization:\n",
    "            mean = volume.mean()\n",
    "            std = volume.std()\n",
    "            \n",
    "            if std>0:\n",
    "                volume = (volume - mean) / std\n",
    "            else:\n",
    "                volume = torch.zeros_like(volume)\n",
    "        #3채널은 아니잖아요?\n",
    "        volume = volume.unsqueeze(0)\n",
    "        \n",
    "        # regression\n",
    "        # score = torch.tensor(score, dtype=torch.float32)\n",
    "        # return volume, score\n",
    "        \n",
    "        # two_class Classification - CN이 0이고 MCI 이상이 1\n",
    "        label = torch.tensor(0.0 if score >= self.score_threshold else 1.0, dtype=torch.float32)\n",
    "        return volume, label\n",
    "        \n",
    "        # three_class Classification - CN, MCI, AD로 3개로 나눔 (0, 1, 2로 라벨링)\n",
    "        # label = torch.tensor(0.0 if score >= self.score_threshold else (1.0 if score >= self.score_threshold2 else 2.0), dtype=torch.float32)\n",
    "        # return volume, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델정의 여기\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DenseLayer3D(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate, bn_size=4, drop_rate=0):\n",
    "        super(DenseLayer3D, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm3d(in_channels)\n",
    "        self.conv1 = nn.Conv3d(in_channels, bn_size * growth_rate, kernel_size=1, stride=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm3d(bn_size * growth_rate)\n",
    "        self.conv2 = nn.Conv3d(bn_size * growth_rate, growth_rate, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.drop_rate = drop_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        new_features = self.conv1(F.relu(self.bn1(x)))\n",
    "        new_features = self.conv2(F.relu(self.bn2(new_features)))\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
    "        return torch.cat([x, new_features], 1)\n",
    "\n",
    "\n",
    "class DenseBlock3D(nn.Module):\n",
    "    def __init__(self, num_layers, in_channels, growth_rate, bn_size, drop_rate):\n",
    "        super(DenseBlock3D, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            layers.append(DenseLayer3D(\n",
    "                in_channels + i * growth_rate, growth_rate, bn_size, drop_rate\n",
    "            ))\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class TransitionLayer3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(TransitionLayer3D, self).__init__()\n",
    "        self.bn = nn.BatchNorm3d(in_channels)\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n",
    "        self.pool = nn.AvgPool3d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(F.relu(self.bn(x)))\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DenseNet3D(nn.Module):\n",
    "    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16), num_init_features=64, bn_size=4, drop_rate=0, num_classes=2):\n",
    "        super(DenseNet3D, self).__init__()\n",
    "\n",
    "        # Initial convolution\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv3d(1, num_init_features, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm3d(num_init_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "\n",
    "        # Dense Blocks and Transition Layers\n",
    "        num_features = num_init_features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = DenseBlock3D(num_layers, num_features, growth_rate, bn_size, drop_rate)\n",
    "            self.features.add_module(f\"denseblock{i+1}\", block)\n",
    "            num_features += num_layers * growth_rate\n",
    "            if i != len(block_config) - 1:\n",
    "                trans = TransitionLayer3D(num_features, num_features // 2)\n",
    "                self.features.add_module(f\"transition{i+1}\", trans)\n",
    "                num_features = num_features // 2\n",
    "\n",
    "        # Final batch norm\n",
    "        self.features.add_module(\"norm5\", nn.BatchNorm3d(num_features))\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = F.adaptive_avg_pool3d(features, (1, 1, 1)).view(features.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def densenet121_3d(num_classes=2):\n",
    "    return DenseNet3D(\n",
    "        growth_rate=32,\n",
    "        block_config=(6, 12, 24, 16),  # DenseNet-121 구성\n",
    "        num_init_features=64,\n",
    "        bn_size=4,\n",
    "        drop_rate=0,\n",
    "        num_classes=num_classes,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 기본 설정 - 뭐로하든 여기는 동일해야합니다\n",
    "def split_dataset(dataset, train_ratio=0.8):\n",
    "    train_size = int(len(dataset) * train_ratio)\n",
    "    val_size = len(dataset) - train_size\n",
    "    return random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# cuda 안뜨면 다시 설정해줘야함\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 데이터 로더 생성\n",
    "train_dataset_adni = ADNIDataset(train_dataset)  # ADNIDataset 클래스에서 데이터셋 생성\n",
    "label_counts = Counter()\n",
    "for _, label in train_dataset_adni:\n",
    "    label_counts[int(label.item())]+=1\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"{label}:{count}\")    \n",
    "\n",
    "train_set, val_set = split_dataset(train_dataset_adni)  # Train-Val 분리\n",
    "\n",
    "# DataLoader 생성\n",
    "batch_size = 8  # 배치 크기 설정\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#디버깅\n",
    "print(train_dataset_adni[1][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이거는 단순 Regression\n",
    "\n",
    "# 설정\n",
    "\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "writer = SummaryWriter(log_dir=f\"./logs_resnet18_3d/regression/{now.day}-{now.hour}_{now.minute}\")\n",
    "\n",
    "# 모델, 손실 함수, 옵티마이저\n",
    "model = resnet18_3d().to(device)\n",
    "register_hooks_for_tensorboard(model, writer)  # TensorBoard를 위한 Hook 등록\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# 학습 함수\n",
    "def train_model_with_validation_regression(model, train_loader, val_loader, criterion, optimizer, writer, device, epochs):\n",
    "    global global_step\n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training 단계\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "\n",
    "        for i, (volumes, scores) in enumerate(train_loader):\n",
    "            global_step += 1\n",
    "\n",
    "            volumes, scores = volumes.to(device), scores.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(volumes)\n",
    "            loss = criterion(outputs.squeeze(), scores)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "            # TensorBoard에 Training Loss 기록\n",
    "            writer.add_scalar(\"Loss/Train\", loss.item(), global_step)\n",
    "\n",
    "        train_loss = running_train_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        # Validation 단계\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for volumes, scores in val_loader:\n",
    "                volumes, scores = volumes.to(device), scores.to(device)\n",
    "                outputs = model(volumes)\n",
    "                loss = criterion(outputs.squeeze(), scores)\n",
    "                running_val_loss += loss.item()\n",
    "\n",
    "        val_loss = running_val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # TensorBoard에 Validation Loss 기록\n",
    "        writer.add_scalar(\"Loss/Validation\", val_loss, epoch)\n",
    "\n",
    "# 학습\n",
    "epochs = 5  # Epoch 설정\n",
    "train_model_with_validation_regression(model, train_loader, val_loader, criterion, optimizer, writer, device, epochs)\n",
    "\n",
    "# TensorBoard 종료\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two_Class Classification으로 사용할 떄\n",
    "print('device : ', device)\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "writer = SummaryWriter(log_dir=f\"./logs_resnet18_3d/two_class/{now.day}-{now.hour}_{now.minute}_{now.second}\")\n",
    "print(f\"TensorBoard logs are being written to: {writer.log_dir}\")\n",
    "\n",
    "model = densenet121_3d().to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()  # 이진 분류 손실 함수\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "from sklearn.metrics import f1_score, roc_auc_score, roc_curve\n",
    "\n",
    "def train_model_with_validation_two_class(model, train_loader, val_loader, criterion, optimizer, writer, device, epochs):\n",
    "    global global_step\n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training 단계\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "\n",
    "        for i, (volumes, labels) in enumerate(train_loader):  # scores -> labels\n",
    "            global_step += 1\n",
    "\n",
    "            volumes, labels = volumes.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(volumes).squeeze()  # [batch_size, 1]\n",
    "\n",
    "            loss = criterion(outputs, labels.float())  # BCEWithLogitsLoss expects float labels\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "            # TensorBoard에 Training Loss 기록\n",
    "            writer.add_scalar(\"Loss/Train\", loss.item(), global_step)\n",
    "\n",
    "        train_loss = running_train_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        # Validation 단계\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        all_labels = []\n",
    "        all_outputs = []\n",
    "        with torch.no_grad():\n",
    "            for volumes, labels in val_loader:  # scores -> labels\n",
    "                volumes, labels = volumes.to(device), labels.to(device)\n",
    "                outputs = model(volumes).squeeze()  # [batch_size, 1]\n",
    "                loss = criterion(outputs, labels.float())\n",
    "                running_val_loss += loss.item()\n",
    "\n",
    "                # F1-score 및 ROC-AUC 계산을 위한 값 저장\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_outputs.extend(torch.sigmoid(outputs).cpu().numpy())\n",
    "\n",
    "        val_loss = running_val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # TensorBoard에 Validation Loss 기록\n",
    "        writer.add_scalar(\"Loss/Validation\", val_loss, epoch)\n",
    "\n",
    "        # F1-score 계산 및 기록\n",
    "        val_f1 = f1_score(all_labels, (np.array(all_outputs) > 0.5).astype(int))\n",
    "        writer.add_scalar(\"Metrics/F1_Score\", val_f1, epoch)\n",
    "\n",
    "        # ROC-AUC 계산 및 기록\n",
    "        val_roc_auc = roc_auc_score(all_labels, all_outputs)\n",
    "        writer.add_scalar(\"Metrics/ROC_AUC\", val_roc_auc, epoch)\n",
    "\n",
    "        # ROC Curve 계산 및 TensorBoard에 기록\n",
    "        fpr, tpr, _ = roc_curve(all_labels, all_outputs)\n",
    "        writer.add_pr_curve(\"ROC_Curve\", torch.tensor(fpr), torch.tensor(tpr), epoch)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, F1 Score: {val_f1:.4f}, ROC AUC: {val_roc_auc:.4f}\")\n",
    "epochs = 20\n",
    "train_model_with_validation_two_class(model, train_loader, val_loader, criterion, optimizer, writer, device, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'resnet3d_18_epoch30.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three_class Classification일 경우\n",
    "# 모델, 손실 함수, 옵티마이저\n",
    "writer = SummaryWriter(log_dir=f\"./logs_resnet18_3d/three_class/{now.day}-{now.hour}_{now.minute}\")\n",
    "model = resnet18_3d().to(device)\n",
    "register_hooks_for_tensorboard(model, writer)  # TensorBoard를 위한 Hook 등록\n",
    "criterion = nn.CrossEntropyLoss()  # 다중 클래스 분류 손실 함수\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "# 학습 함수\n",
    "def train_model_with_validation_three_class(model, train_loader, val_loader, criterion, optimizer, writer, device, epochs):\n",
    "    global global_step\n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training 단계\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "\n",
    "        for i, (volumes, labels) in enumerate(train_loader): \n",
    "            global_step += 1\n",
    "\n",
    "            volumes, labels = volumes.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(volumes) \n",
    "            loss = criterion(outputs, labels)  \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "            # TensorBoard에 Training Loss 기록\n",
    "            writer.add_scalar(\"Loss/Train\", loss.item(), global_step)\n",
    "\n",
    "        train_loss = running_train_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        # Validation 단계\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for volumes, labels in val_loader: \n",
    "                volumes, labels = volumes.to(device), labels.to(device)\n",
    "                outputs = model(volumes) \n",
    "                loss = criterion(outputs, labels)\n",
    "                running_val_loss += loss.item()\n",
    "\n",
    "        val_loss = running_val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # TensorBoard에 Validation Loss 기록\n",
    "        writer.add_scalar(\"Loss/Validation\", val_loss, epoch)\n",
    "\n",
    "train_model_with_validation_three_class(model, train_loader, val_loader, criterion, optimizer, writer, device, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18_3d().to(device)\n",
    "model.load_state_dict(torch.load('resnet3d_18_epoch30.pth',weights_only=True),)\n",
    "test_dataset_adni = ADNIDataset(test_dataset)\n",
    "test_loader = DataLoader(test_dataset_adni, batch_size=4, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Test 데이터 평가 및 성능 지표 계산 함수\n",
    "def evaluate_classification_task(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for volumes, labels in test_loader:\n",
    "            volumes, labels = volumes.to(device), labels.to(device)\n",
    "            outputs = model(volumes).squeeze()\n",
    "            preds = torch.sigmoid(outputs)  # BCEWithLogitsLoss의 경우 sigmoid 적용\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # NumPy 배열로 변환\n",
    "    all_preds = np.array(all_preds)\n",
    "    print(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    # 이진 분류 기준 (0.5 threshold)\n",
    "    binary_preds = (all_preds >= 0.5).astype(int)\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(all_labels, binary_preds)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # F1 Score\n",
    "    f1 = f1_score(all_labels, binary_preds)\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # ROC-AUC\n",
    "    roc_auc = roc_auc_score(all_labels, all_preds)\n",
    "    print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(all_labels, binary_preds)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(all_labels, all_preds)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.4f})\")\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Precision-Recall Curve\n",
    "    precision, recall, _ = precision_recall_curve(all_labels, all_preds)\n",
    "    plt.figure()\n",
    "    plt.plot(recall, precision, label=\"Precision-Recall Curve\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision-Recall Curve\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 테스트 데이터셋 평가\n",
    "evaluate_classification_task(model, test_loader, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nrf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
